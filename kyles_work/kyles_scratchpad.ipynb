{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351a8086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ahocorasick\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8017b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/kylegreen/codeup-data-science/trials_in_rust/trials-in-rust/first_rust_program/Testing/Almustaqbal.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1e9872",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1).headline.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faeacfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4afa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1).text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81933c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "from camel_tools.sentiment import SentimentAnalyzer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb987e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "from camel_tools.sentiment import SentimentAnalyzer\n",
    "from transformers import pipeline\n",
    "\n",
    "#mix = pipeline('text-classification', model=\"CAMeL-Lab/bert-base-arabic-camelbert-mix-sentiment\")\n",
    "#ca = pipeline('text-classification', model=\"CAMeL-Lab/bert-base-arabic-camelbert-ca-sentiment\")\n",
    "#da = pipeline('text-classification', model=\"CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment\")\n",
    "msa = pipeline('text-classification', model=\"CAMeL-Lab/bert-base-arabic-camelbert-msa-sentiment\")\n",
    "\n",
    "#half = pipeline('text-classification', model=\"CAMeL-Lab/bert-base-arabic-camelbert-msa-half\")\n",
    "#quarter = pipeline('text-classification', model=\"CAMeL-Lab/bert-base-arabic-camelbert-msa-quarter\")\n",
    "#eighth = pipeline('text-classification', model=\"CAMeL-Lab/bert-base-arabic-camelbert-msa-eighth\")\n",
    "#sixteenth = pipeline('text-classification', model=\"CAMeL-Lab/bert-base-arabic-camelbert-msa-sixteenth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839ae015",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = (mix, ca, da, msa, half, quarter, eighth, sixteenth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ddd302",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mod in models:\n",
    "    print(mod('hello'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96cb2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5).text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7248e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(val.split()) for val in df.head(100).text.values])/100\n",
    "modelable = df.head(2000).text[[True if len(val.split()) < 350 else False for val in df.head(2000).text.values]]\n",
    "for val in modelable.values:\n",
    "    print(msa(val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95bad64",
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in df.head(200).values:\n",
    "    try:\n",
    "        print(msa(val))\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    #for mod in models:\n",
    "        #print(mod(val[:512]))\n",
    "        \n",
    "subset = df.head(100).copy()\n",
    "\n",
    "        \n",
    "        \n",
    "subset['label'] = subset.apply(lambda x:  msa(x.text)[0]['label'] if len(x.text.split()) < 300 else False, axis=1)\n",
    "subset['score'] = subset.apply(lambda x: msa(x.text)[0]['score'] if len(x.text.split()) < 300 else False, axis=1)\n",
    "subset.head()\n",
    "\n",
    "#texts = {}    \n",
    "#for i, val in enumerate(df.head(200).text.values):\n",
    "#    if len(val) > 35 and len(val) < 512:\n",
    "#        texts[i] = val\n",
    "        \n",
    "        \n",
    "#print(texts)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b269344",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cfd963",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.score.groupby(subset.label).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0579bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset[subset.label== 'negative'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2db8bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506c90c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_to_english(text):\n",
    "    translation = translator.translate(text, dest='en').text\n",
    "    return translation\n",
    "\n",
    "translate_to_english(subset.head(1).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.to_csv('labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4ed2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('labels.csv')\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4337a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "def translate_to_english(text):\n",
    "    translation = translator.translate(text, dest='en').text\n",
    "    return translation\n",
    "\n",
    "def check_for_america(translation):\n",
    "    if 'america' in translation.lower() or 'united states' in translation.lower():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "text = sub.head(1).text\n",
    "\n",
    "\n",
    "\n",
    "#translation = translate_to_english(text)\n",
    "#check_for_america(translation)\n",
    "\n",
    "row_nums = df.shape[0]\n",
    "sets_of_ten = int(row_nums / 100)\n",
    "\n",
    "collection_of_dfs = []\n",
    "print(sets_of_ten)\n",
    "\n",
    "for i in range(0,100):\n",
    "    collection_of_dfs.append(df[df.index.isin(range(i * sets_of_ten, (i+1) * sets_of_ten))].copy())\n",
    "    collection_of_dfs[i]['america'] = [True if val.str.contains('')]\n",
    "    print(collection_of_dfs[f'frame_{i}'])\n",
    "    break\n",
    "    \n",
    "\n",
    "#sub['en_text'] = sub.apply(lambda x: translate_to_english(x.text), axis=1)\n",
    "#sub.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45899ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edbf011",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['source'] = 'almustaqbal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f857d743",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[mask].to_csv('almustaqbal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd33e802",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def flip_key_value_pairs(dicts):\n",
    "    res = dict((v,k) for k,v in dicts.items())\n",
    "    return res\n",
    "\n",
    "def using_ahocorasick(col, lst):\n",
    "    A = ahocorasick.Automaton(ahocorasick.STORE_INTS)\n",
    "    for word in lst:\n",
    "        A.add_word(word.lower())\n",
    "    A.make_automaton() \n",
    "    col = col.astype(str)\n",
    "    col = col.str.lower()\n",
    "    mask = col.apply(lambda x: bool(list(A.iter(x))))\n",
    "    tags = col.apply(lambda x: list(A.iter(x)))\n",
    "    return mask, tags\n",
    "\n",
    "def look_for_words_in_text(df_text):\n",
    "    \n",
    "    topics = {'America' : 'أمريكا',\n",
    "            'American' : 'أمريكيّ',\n",
    "            'American (f)' : 'أمريكيّة',\n",
    "            'American (pl)' : 'أمريكيّين',\n",
    "            'The United States' : 'الولايات المتحدة',\n",
    "            'The United States' : 'دول موحّدة',\n",
    "            'Washington' : 'واشنطن',\n",
    "            'Bush' : 'بوش',\n",
    "            'Obama' : 'أوباما',\n",
    "            'Cheney' : 'تشيني',\n",
    "            'Clinton' : 'كلينتون',\n",
    "            'Osama Bin Laden' : 'أسامة بن لادن',\n",
    "            'Al Gore' : 'آل غور',\n",
    "            'World Trade Center' : 'مركز التجارة العالمي',\n",
    "            '9/11' : '9/11',\n",
    "            'September 11' : '11 سبتمبر',\n",
    "            'Gulf War' : 'حرب الخليج',\n",
    "            'Google' : 'غوغل',\n",
    "            'Facebook' : 'فيسبوك',\n",
    "            'Al Qaida' : 'القاعدة'}\n",
    "    \n",
    "    topics = flip_key_value_pairs(topics)\n",
    "    \n",
    "    tags = []\n",
    "    for key in topics.keys():\n",
    "        if key in df_text:\n",
    "            tags.append(key)\n",
    "            \n",
    "    return tags\n",
    "\n",
    "def make_relevant_tagged_df(df):\n",
    "    topics = {'America' : 'أمريكا',\n",
    "            'American' : 'أمريكيّ',\n",
    "            'American (f)' : 'أمريكيّة',\n",
    "            'American (pl)' : 'أمريكيّين',\n",
    "            'The United States' : 'الولايات المتحدة',\n",
    "            'The United States' : 'دول موحّدة',\n",
    "            'Washington' : 'واشنطن',\n",
    "            'Bush' : 'بوش',\n",
    "            'Obama' : 'أوباما',\n",
    "            'Cheney' : 'تشيني',\n",
    "            'Clinton' : 'كلينتون',\n",
    "            'Osama Bin Laden' : 'أسامة بن لادن',\n",
    "            'Al Gore' : 'آل غور',\n",
    "            'World Trade Center' : 'مركز التجارة العالمي',\n",
    "            '9/11' : '9/11',\n",
    "            'September 11' : '11 سبتمبر',\n",
    "            'Gulf War' : 'حرب الخليج',\n",
    "            'Google' : 'غوغل',\n",
    "            'Facebook' : 'فيسبوك',\n",
    "            'Al Qaida' : 'القاعدة'}\n",
    "\n",
    "\n",
    "    topics = flip_key_value_pairs(topics)\n",
    "    mask, tags = using_ahocorasick(df.text, list(topics.keys()))\n",
    "\n",
    "    copied = df[mask].copy()\n",
    "\n",
    "    copied['tags'] = copied.text.apply(look_for_words_in_text)\n",
    "\n",
    "    return copied\n",
    "\n",
    "publication_names = ['Alittihad', 'Almasryalyoum', 'Almustaqbal', 'Alqabas', 'Echoroukonline', 'Ryiadh', 'Sabanews', 'SaudiYoum', 'Techreen', 'Youm7']\n",
    "\n",
    "filepath = '/Users/kylegreen/codeup-data-science/trials_in_rust/trials-in-rust/first_rust_program/Testing/'\n",
    "\n",
    "for name in publication_names:\n",
    "    path = filepath+name\n",
    "    df = pd.read_csv(path + '.csv')\n",
    "    df = make_relevant_tagged_df(df)\n",
    "    df['source'] = name\n",
    "    df.to_csv(name+'.csv', index=False)\n",
    "    print(name)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea915ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in copied.tags.values:\n",
    "    if len(val) > 1:\n",
    "        print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0280913",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sub['america'] = sub.apply(lambda x: check_for_america(x.en_text), axis=1)\n",
    "sub.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d0a181",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub[sub.america==True].label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39c4fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(publication_names[4] + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfea368",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvs = []\n",
    "for name in publication_names:\n",
    "    csvs.append(pd.read_csv(name + '.csv'))\n",
    "df = pd.concat(csvs)\n",
    "df.to_csv('all_articles.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a647f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.source.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7915a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524b7635",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(500).to_csv('sample_articles.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa6cd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = int(df.shape[0] /3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb67c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dfs = []\n",
    "for i in range(0, df.shape[0] - steps, steps):\n",
    "    split_dfs.append(df.iloc[i: i+steps])\n",
    "    \n",
    "split_dfs[0].text.unique().size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efb4c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6380cbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(split_dfs):\n",
    "    d.to_csv(f'block_{i+1}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b51510",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dfs[2].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d066d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline('summarization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1011b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_msa(df_text):\n",
    "    try:\n",
    "        done = msa(df_text)\n",
    "        return done\n",
    "    except:\n",
    "        try:\n",
    "            first_half = msa(df_text[:round(len(df_text)/2)]) \n",
    "            second_half = msa(df_text[round(len(df_text)/2):])\n",
    "            if first_half[0]['label'] == second_half[0]['label']:\n",
    "                label = first_half[0]['label']\n",
    "                score = (test_1[0]['score'] + test_2[0]['score'])/2\n",
    "                done = [{'label': label, 'score': score}]\n",
    "                return done\n",
    "        except:\n",
    "            print(df_text)\n",
    "            return False\n",
    "        \n",
    "\n",
    "def analyze_text(df):\n",
    "    scores = []\n",
    "    scores = df.iloc[0:50].text.apply(make_msa)\n",
    "    #df['score'] = df.text.apply(lambda x: msa(x)[0]['score'] if len(x.split()) < 300 else False)\n",
    "    return scores\n",
    "\n",
    "def analyze_headline(df):\n",
    "    headline_scores = []\n",
    "    scores = df.iloc[0:200].headline.apply(make_msa)\n",
    "    return scores\n",
    "\n",
    "#(analyze_text(df) != False).sum()\n",
    "(analyze_text(subset_df) != False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46cd148",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= 'ائيل عن وثائق جديدة اليوم، السبت، عن أسرار حرب الاستنزاف التى خاضتها مصر ض'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9684af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = pd.read_csv('sample_articles.csv')\n",
    "subset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a16c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2708bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in subset_df.text.values:\n",
    "    if text in val:\n",
    "        print(subset_df[subset_df.text==val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a14d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dicts = {}\n",
    "for source in df.source.unique():\n",
    "    source_dicts[source] = df[df.source == source]\n",
    "    \n",
    "source_dicts['SaudiYoum']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e373c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8031e781",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dateline.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e43e88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in list(df.dateline.values):\n",
    "    try:\n",
    "        print(re.search('20\\d\\d\\W\\d\\d\\W\\d\\d', val).group())\n",
    "        \n",
    "    except:\n",
    "        print(val)\n",
    "\n",
    "#print(len(df.dateline.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ff81df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6766b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dicts['SaudiYoum'].dateline.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d209b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def using_ahocorasick_months(col, lst):\n",
    "    A = ahocorasick.Automaton(ahocorasick.STORE_INTS)\n",
    "    for word in lst:\n",
    "        A.add_word(word.lower())\n",
    "    A.make_automaton() \n",
    "    col = col.astype(str)\n",
    "    col = col.str.lower()\n",
    "    mask = col.apply(lambda x: bool(list(A.iter(x))))\n",
    "    return mask\n",
    "\n",
    "def look_for_words_in_text_months(df_text):\n",
    "    \n",
    "    months = {'January' : 'يناير',\n",
    "                        'February' : 'فبراير',\n",
    "                        'March' : 'مارس',\n",
    "                        'April' : 'أبريل',\n",
    "                        'June' : 'يوني',\n",
    "                        'July' : 'يولي',\n",
    "                        'August' : 'أغسطس',\n",
    "                        'September' : 'سبتمبر',\n",
    "                        'October' : 'أكتوبر',\n",
    "                        'November' : 'نوفمبر',\n",
    "                        'December' : 'ديسمبر',\n",
    "                        'April1' : 'إبريل',\n",
    "                        'May': 'ماي'}\n",
    "    \n",
    "    months = flip_key_value_pairs(months)\n",
    "    \n",
    "    tags = []\n",
    "    for key in months.keys():\n",
    "        if key in df_text:\n",
    "            tags.append(key)\n",
    "            \n",
    "    return tags\n",
    "\n",
    "mask = using_ahocorasick(df.dateline, list(months.values()))[0]\n",
    "copy = df[mask].copy()\n",
    "copy = pd.DataFrame(copy)\n",
    "copy['month'] = copy.dateline.apply(look_for_words_in_text_months)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81542df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a193de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_month(df_dateline):\n",
    "    exp = '\\d\\d.*\\d\\d\\d\\d'\n",
    "    try:\n",
    "        month = re.search(exp, df_dateline).group()[2:-4]\n",
    "        months = {'January' : 'يناير',\n",
    "                        'February' : 'فبراير',\n",
    "                        'March' : 'مارس',\n",
    "                        'April' : 'أبريل',\n",
    "                        'June' : 'يوني',\n",
    "                        'July' : 'يولي',\n",
    "                        'August' : 'أغسطس',\n",
    "                        'September' : 'سبتمبر',\n",
    "                        'October' : 'أكتوبر',\n",
    "                        'November' : 'نوفمبر',\n",
    "                        'December' : 'ديسمبر',\n",
    "                        'April1' : 'إبريل',\n",
    "                        'May': 'ماي'}\n",
    "    \n",
    "        months = flip_key_value_pairs(months)\n",
    "        \n",
    "        return re.search(exp, df_dateline).group()[2:-4]\n",
    "    except:\n",
    "        \n",
    "        try:\n",
    "            return re.search('20\\d\\d\\W\\d\\d\\W\\d\\d', val).group()\n",
    "\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "df.dateline.apply(find_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec83ecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dateline.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce080410",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('all_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a3cf033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "from camel_tools.sentiment import SentimentAnalyzer\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Change name to which file you're doing sentiment analysis on, NOT including the '.csv' \n",
    "Output file will be 'labeled_<filename>.csv'\n",
    "\"\"\"\n",
    "\n",
    "msa = pipeline('text-classification', model=\"CAMeL-Lab/bert-base-arabic-camelbert-msa-sentiment\")\n",
    "\n",
    "\n",
    "\n",
    "def load_and_label_df(name):\n",
    "    path = 'split_articles/'\n",
    "    df = load_csv(path + name)\n",
    "    print(f'loaded {name}')\n",
    "    print('labeling/scoring...')\n",
    "    df = create_labels_scores(df, name)\n",
    "    print('done labeling/scoring!')\n",
    "    return df\n",
    "\n",
    "def load_csv(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    return df\n",
    "\n",
    "def make_msa(df_text):\n",
    "    \n",
    "    try:\n",
    "        done = msa(df_text)\n",
    "        return done\n",
    "    except:\n",
    "        \n",
    "        try:\n",
    "            first_half = msa(df_text[:round(len(df_text)/2)]) \n",
    "            second_half = msa(df_text[round(len(df_text)/2):])\n",
    "            if first_half[0]['label'] == second_half[0]['label']:\n",
    "                label = first_half[0]['label']\n",
    "                score = (test_1[0]['score'] + test_2[0]['score'])/2\n",
    "            done = [{'label': label, 'score': score}]\n",
    "            return done\n",
    "        except:\n",
    "        \n",
    "            try:\n",
    "                beginning = msa(df_text[:round(len(df_text)/3)]) \n",
    "                middle = msa(df_text[round(len(df_text)/3):round(len(df_text)*2/3)])\n",
    "                end = msa(df_text[round(len(df_text)*2/3):])\n",
    "\n",
    "                if (beginning[0]['label'] == middle[0]['label']) and (beginning[0]['label'] == end[0]['label']):\n",
    "                    label = first_half[0]['label']\n",
    "                    score = (beginning[0]['score'] + middle[0]['score'] + end[0]['score'])/3\n",
    "                    done = [{'label': label, 'score': score}]\n",
    "                    return done\n",
    "                else:\n",
    "                    return 'sentiment of parts not equal'\n",
    "            except:\n",
    "                return '3 is not enough'\n",
    "        \n",
    "def analyze_text(df):\n",
    "    scores = []\n",
    "    print('analyzing_texts')\n",
    "    scores = [make_msa(val) for val in df.text.values]\n",
    "    return scores\n",
    "\n",
    "def analyze_headline(df):\n",
    "    print('analyzing headlines')\n",
    "    headline_scores = [make_msa(val) for val in df.headline.values]\n",
    "    return headline_scores\n",
    "\n",
    "def label_and_scores(msa_scores):\n",
    "    labels = []\n",
    "    scores = []\n",
    "    for val in msa_scores:\n",
    "        try:\n",
    "            labels.append(val[0]['label'])\n",
    "            scores.append(val[0]['score'])\n",
    "        except:\n",
    "            labels.append(False)\n",
    "            scores.append(False)\n",
    "\n",
    "    return labels, scores\n",
    "\n",
    "def create_labels_scores(df, name):\n",
    "    text_scores = analyze_text(df)\n",
    "    labels, scores = label_and_scores(text_scores)\n",
    "    df['text_label'] = labels\n",
    "    df['text_score'] = scores\n",
    "\n",
    "    headline_scores = analyze_headline(df)\n",
    "    labels, scores = label_and_scores(headline_scores)\n",
    "    df['headline_label'] = labels\n",
    "    df['headline_score'] = scores\n",
    "\n",
    "    # CHANGE 'BLOCK_NAME' TO WHATEVER YOU WANT\n",
    "    df.to_csv('labeled_split_articles/labeled_'+ name + '.csv', index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "#load_and_label_df(name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4335486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('labeled_' +name+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faee65ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tons_of_csvs():\n",
    "    df = pd.read_csv('/Users/kylegreen/codeup-data-science/all_articles.csv')\n",
    "    \n",
    "    end_point = '/Users/kylegreen/codeup-data-science/arabic_media_nlp_project/kyles_work/split_articles/'\n",
    "    \n",
    "    df_shape = df.shape[0]\n",
    "    \n",
    "    one_thousandth = int(df_shape/1000)\n",
    "    \n",
    "    for i in range(0, df_shape, one_thousandth):\n",
    "        copy = df.iloc[i:i+one_thousandth].copy()\n",
    "        copy.to_csv(end_point+str(i) + '.csv', index=False)\n",
    "    \n",
    "split_tons_of_csvs().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4f890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('split_articles/354501.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13db54bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc34035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['328083', '274890', '167076', '188853', '326655', '194922', '272748', '39627', '48552', '354501', '158508', '207060', '11424', '236334', '81396', '20349', '267393', '50337', '338079', '72828', '191709', '33915', '6783', '103530', '268464', '91392', '79968', '193137', '261324', '184926', '261681', '18207', '240261', '121023', '17493', '74256', '154938', '264894', '228123', '336651', '51765', '158865', '25347', '89250', '168147', '149940', '353787', '277746', '130662', '201348', '343434', '347718', '297738', '26061', '325941', '162078', '94605', '357', '12138', '124950', '142800', '334866', '187425', '234906', '327012', '186711', '35343', '225981', '54621', '121380']\n",
      "loaded 255255.csv\n",
      "labeling/scoring...\n",
      "analyzing_texts\n",
      "analyzing headlines\n",
      "done labeling/scoring!\n",
      "loaded 131019.csv\n",
      "labeling/scoring...\n",
      "analyzing_texts\n",
      "analyzing headlines\n",
      "done labeling/scoring!\n",
      "loaded 120666.csv\n",
      "labeling/scoring...\n",
      "analyzing_texts\n",
      "analyzing headlines\n",
      "done labeling/scoring!\n",
      "loaded 271320.csv\n",
      "labeling/scoring...\n",
      "analyzing_texts\n",
      "analyzing headlines\n",
      "done labeling/scoring!\n",
      "loaded 355929.csv\n",
      "labeling/scoring...\n",
      "analyzing_texts\n",
      "analyzing headlines\n",
      "done labeling/scoring!\n",
      "loaded 318801.csv\n",
      "labeling/scoring...\n",
      "analyzing_texts\n",
      "analyzing headlines\n",
      "done labeling/scoring!\n",
      "loaded 169932.csv\n",
      "labeling/scoring...\n",
      "analyzing_texts\n",
      "analyzing headlines\n",
      "done labeling/scoring!\n",
      "loaded 48195.csv\n",
      "labeling/scoring...\n",
      "analyzing_texts\n",
      "analyzing headlines\n",
      "done labeling/scoring!\n",
      "loaded 302736.csv\n",
      "labeling/scoring...\n",
      "analyzing_texts\n",
      "analyzing headlines\n",
      "done labeling/scoring!\n",
      "loaded 69615.csv\n",
      "labeling/scoring...\n",
      "analyzing_texts\n",
      "analyzing headlines\n",
      "done labeling/scoring!\n",
      "loaded 334509.csv\n",
      "labeling/scoring...\n",
      "analyzing_texts\n",
      "analyzing headlines\n",
      "done labeling/scoring!\n",
      "loaded 3570.csv\n",
      "labeling/scoring...\n",
      "analyzing_texts\n",
      "analyzing headlines\n",
      "done labeling/scoring!\n",
      "loaded 350931.csv\n",
      "labeling/scoring...\n",
      "analyzing_texts\n",
      "analyzing headlines\n",
      "done labeling/scoring!\n",
      "loaded 98175.csv\n",
      "labeling/scoring...\n",
      "analyzing_texts\n",
      "analyzing headlines\n",
      "done labeling/scoring!\n",
      "loaded 280245.csv\n",
      "labeling/scoring...\n",
      "analyzing_texts\n",
      "analyzing headlines\n",
      "done labeling/scoring!\n",
      "loaded 236691.csv\n",
      "labeling/scoring...\n",
      "analyzing_texts\n",
      "analyzing headlines\n",
      "done labeling/scoring!\n",
      "loaded 17136.csv\n",
      "labeling/scoring...\n",
      "analyzing_texts\n",
      "analyzing headlines\n",
      "done labeling/scoring!\n",
      "loaded 238119.csv\n",
      "labeling/scoring...\n",
      "analyzing_texts\n",
      "analyzing headlines\n",
      "done labeling/scoring!\n",
      "loaded 77826.csv\n",
      "labeling/scoring...\n",
      "analyzing_texts\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "\n",
    "labeled = 'labeled_split_articles'\n",
    "unlabeled = 'split_articles'\n",
    "\n",
    "os.listdir(labeled)\n",
    "\n",
    "unlabeled_files = os.listdir(unlabeled)\n",
    "labeled_files = []\n",
    "\n",
    "for path in os.listdir(labeled):\n",
    "    labeled_files.append(re.search('\\d*\\.', path).group()[:-1])\n",
    "\n",
    "print(labeled_files)\n",
    "    \n",
    "    \n",
    "ul_files = unlabeled_files[:]\n",
    "files_found  = []\n",
    "for p in labeled_files:\n",
    "    for pth in unlabeled_files:\n",
    "        if p+'.csv' == pth:\n",
    "            ul_files.remove(pth)\n",
    "\n",
    "path = 'split_articles/'\n",
    "for file in ul_files:\n",
    "    load_and_label_df(file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da4a888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "labeled = 'labeled_split_articles'\n",
    "\n",
    "\n",
    "\n",
    "bunk_files = []\n",
    "\n",
    "for fi in os.listdir(labeled):\n",
    "    df = pd.read_csv(labeled + '/' + fi)\n",
    "    if df.text_score.sum() == 0:\n",
    "        bunk_files.append(fi)\n",
    "        \n",
    "for fi in bunk_files:\n",
    "    if os.path.isfile(f'{labeled}/{fi}'):\n",
    "        os.remove(f'{labeled}/{fi}')\n",
    "    else:\n",
    "        print(f\"{fi} doesn't exist\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1230f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import pandas as pd\n",
    "\n",
    "def find_month(df_dateline):\n",
    "    exp = '\\d\\d.*\\d\\d\\d\\d'\n",
    "    try:\n",
    "        month = re.search(exp, df_dateline).group()[2:-4]\n",
    "        months = {'January' : 'يناير',\n",
    "                        'February' : 'فبراير',\n",
    "                        'March' : 'مارس',\n",
    "                        'April' : 'أبريل',\n",
    "                        'June' : 'يوني',\n",
    "                        'July' : 'يولي',\n",
    "                        'August' : 'أغسطس',\n",
    "                        'September' : 'سبتمبر',\n",
    "                        'October' : 'أكتوبر',\n",
    "                        'November' : 'نوفمبر',\n",
    "                        'December' : 'ديسمبر',\n",
    "                        'April1' : 'إبريل',\n",
    "                        'May': 'ماي'}\n",
    "    \n",
    "        months = flip_key_value_pairs(months)\n",
    "        \n",
    "        return re.search(exp, df_dateline).group()[2:-4]\n",
    "    except:\n",
    "        \n",
    "        try:\n",
    "            return re.search('20\\d\\d\\W\\d\\d\\W\\d\\d', val).group()\n",
    "\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "\n",
    "def flip_key_value_pairs(dicts):\n",
    "    res = dict((v,k) for k,v in dicts.items())\n",
    "    return res\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36311d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('labeled_split_articles/labeled_312732.csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e457341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/kylegreen/codeup-data-science/arabic_media_nlp_project/kyles_work/labeled_split_articles'\n",
    "name = 'labeled_187425.csv.csv'\n",
    "filepath = path+'/'+name\n",
    "\n",
    "pd.read_csv(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c0aa2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ebd68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
