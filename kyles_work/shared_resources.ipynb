{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602c9273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import regex as re\n",
    "from datetime import datetime\n",
    "\n",
    "import modeling as md\n",
    "import acquire\n",
    "import exploration\n",
    "import split\n",
    "import prepare\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28261d33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_articles = pd.read_csv('all_articles.csv')\n",
    "all_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ed90fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_time = datetime.now()\n",
    "\n",
    "daily = pd.read_csv(f'daily{curr_time.month}_{curr_time.day}.csv')\n",
    "\n",
    "daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aa0b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily.source.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d478d5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import date_fixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820c4353",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_df = date_fixer.make_datetime(daily).drop(columns=['index', 'Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f128db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fixed_df = fixed_df.set_index('dateline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7306a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a513c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rev_topics = flip_key_value_pairs(topics)\n",
    "print(rev_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39d181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topics = {'America' : 'أمريكا',\n",
    "            'American' : 'أمريكيّ',\n",
    "            'American (f)' : 'أمريكيّة',\n",
    "            'American (pl)' : 'أمريكيّين',\n",
    "            'The United States' : 'الولايات المتحدة',\n",
    "            'The United States' : 'دول موحّدة',\n",
    "            'Washington' : 'واشنطن',\n",
    "            'Bush' : 'بوش',\n",
    "            'Obama' : 'أوباما',\n",
    "            'Cheney' : 'تشيني',\n",
    "            'Clinton' : 'كلينتون',\n",
    "            'Osama Bin Laden' : 'أسامة بن لادن',\n",
    "            'Al Gore' : 'آل غور',\n",
    "            'World Trade Center' : 'مركز التجارة العالمي',\n",
    "            '9/11' : '9/11',\n",
    "            'September 11' : '11 سبتمبر',\n",
    "            'Gulf War' : 'حرب الخليج',\n",
    "            'Google' : 'غوغل',\n",
    "            'Facebook' : 'فيسبوك',\n",
    "            'Al Qaida' : 'القاعدة'}\n",
    "\n",
    "\n",
    "def flip_key_value_pairs(dicts):\n",
    "    \n",
    "    res = dict((v,k) for k,v in dicts.items())    \n",
    "    return res\n",
    "\n",
    "\n",
    "def make_eng_tags(df_tags):\n",
    "    eng_tags =[]\n",
    "    \n",
    "    for key, value in rev_topics.items():\n",
    "        if key in df_tags:\n",
    "            eng_tags.append(value)\n",
    "            \n",
    "    return eng_tags\n",
    "\n",
    "fixed_df.tags.apply(make_eng_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e12b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/kylegreen/My_Drive/arabic_NLP/daily8_4.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee51c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_table=pd.DataFrame(\n",
    "    {\n",
    "        'source': ['Alittihad','Echoroukonline','Ryiadh','SaudiYoum','Techreen', 'Alqabas', 'Almustaqbal','Almasryalyoum', 'Youm7','Sabanews'],\n",
    "        'year': [ 1969 , 1991 , 1965 , 1965 , 1975 ,  1972 ,  1999 , 2004 , 2008 , 1990]\n",
    "    }\n",
    ")\n",
    "mapping = transcription_table.set_index('source').to_dict()['year']\n",
    "df['source_start_year'] = df['source'].apply(lambda x: mapping.get(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac6580",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86927dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  df[df.text_label != 'False']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_tagger(df):\n",
    "    country_map = { 'Alittihad': 'emirates',\n",
    "                    'Echoroukonline': 'algeria',\n",
    "                    'Ryiadh': 'ksa',\n",
    "                    'SaudiYoum': 'ksa',\n",
    "                    'Techreen': 'syria',\n",
    "                    'Alqabas': 'kuwait',\n",
    "                    'Almustaqbal': 'lebanon',\n",
    "                    'Almasryalyoum': 'egypt',\n",
    "                    'Youm7': 'egypt',\n",
    "                    'Sabanews': 'yemen',\n",
    "                    }\n",
    "    df['country'] = df.source.map(country_map)\n",
    "    return df\n",
    "\n",
    "df = country_tagger(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d7bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tags = []\n",
    "for tag in df.tags.values:\n",
    "    list_of_tags.extend([val[1:-1] for val in tag[1:-1].split(', ')])\n",
    "    \n",
    "tag_list = list(set(list_of_tags))\n",
    "\n",
    "for tag in tag_list:\n",
    "    df[tag] = 0\n",
    "\n",
    "for i, tag in enumerate(df.tags):\n",
    "    for t in tag_list:\n",
    "        if t in tag:\n",
    "            df[t].iloc[i] = 1\n",
    "            \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83237dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99badf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[tag_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a04f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_values(df, columns_to_encode):\n",
    "    '''\n",
    "    This function takes in a prepared dataframe and using one-hot encoding, encodes categorical variables. It does not drop the original\n",
    "    categorical columns. This is done purposefully to allow for easier Exploratory Data Analysis.  Removal of original categorical columns\n",
    "    will be done in a separate function later if desired.\n",
    "    Parameters: df - a prepared dataframe with the expected feature names and columns\n",
    "    Returns: encoded - a dataframe with all desired categorical columns encoded.\n",
    "    '''\n",
    "    dummies_list = columns_to_encode\n",
    "\n",
    "    dummy_df = pd.get_dummies(df[dummies_list], drop_first=False)\n",
    "    encoded = pd.concat([df, dummy_df], axis = 1)\n",
    "    return encoded\n",
    "\n",
    "encoded = encode_values(df, ['source', 'country'])\n",
    "encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61f5253",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[['dateline', 'source', 'country', 'tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c97a99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['scaled_date'] = (df.dateline.astype('datetime64') - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43451af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.scaled_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db31773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(df[['scaled_date', 'source_start_year']])\n",
    "\n",
    "df[['scaled_date', 'source_start_year_scaled']] = scaler.transform(df[['scaled_date', 'source_start_year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ed53a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6b4de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4483026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_to_drop = ['index', 'id', 'url', 'headline', 'dateline', 'text', 'tags', 'source', 'text_score', 'headline_label', 'headline_score',\n",
    "       'source_start_year', 'country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded = encoded.drop(columns=tags_to_drop)\n",
    "encoded = encoded.rename(columns={'scaled_date': 'scaled_pub_date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bf655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d7bafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = encoded.drop(columns=tags_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aff2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.to_csv('encoded_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313992fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = split.train_validate_test_split(encoded, 'text_label')\n",
    "\n",
    "models = md.all_reports(train, validate, test, 'text_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b9d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "md.Results.total_summary.sort_values(by='validate_accuracy', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52305d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original dataframe\n",
    "df.text_label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813b0724",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_articles = df.shape[0]\n",
    "baseline_correct_articles = 0.732755 * num_articles\n",
    "model_correct_articles = 0.743429 * num_articles0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bec08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num of articles correctly identified by our model compared to baseline\n",
    "int(model_correct_articles - baseline_correct_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66577e3",
   "metadata": {},
   "source": [
    "Our model performs 1.46% better than baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cdc83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "md.Results.total_summary.to_csv('model_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29637cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('model_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aabce8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.sort_values(by='validate_accuracy', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d8239",
   "metadata": {},
   "outputs": [],
   "source": [
    "md.random_forests(train, validate, test, 'text_label', min_sample_leaf=4, depth=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bc51ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('daily8_5.csv')\n",
    "df.head(), df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fed75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df[df.text_label != 'False']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aee377",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb6f254",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('final_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0346234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6f6217",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.iloc[331637]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f8e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[331637, df.columns.get_loc('dateline')] = '2008-04-02 00:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2daffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49564b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df[df.text_label !='False'].drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac0c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a5ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('final_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4965b4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "836e22b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def encode_data():\n",
    "\n",
    "    df = pd.read_csv('final_df.csv')\n",
    "    df.dateline = df.dateline.astype('datetime64')\n",
    "\n",
    "    def is_gov_controlled(entry):\n",
    "        if entry in ['Alqabas', 'Echoroukonline', 'Ryiadh', 'Saudiyoum', 'Almustaqbal', 'Youm7', 'Almasryalyoum']:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    df['ownership_status'] = df.source.apply(is_gov_controlled)\n",
    "\n",
    "\n",
    "    def encode_values(df, columns_to_encode):\n",
    "        '''\n",
    "        This function takes in a prepared dataframe and using one-hot encoding, encodes categorical variables. It does not drop the original\n",
    "        categorical columns. This is done purposefully to allow for easier Exploratory Data Analysis.  Removal of original categorical columns\n",
    "        will be done in a separate function later if desired.\n",
    "        Parameters: df - a prepared dataframe with the expected feature names and columns\n",
    "        Returns: encoded - a dataframe with all desired categorical columns encoded.\n",
    "        '''\n",
    "        dummies_list = columns_to_encode\n",
    "\n",
    "        dummy_df = pd.get_dummies(df[dummies_list], drop_first=False)\n",
    "        encoded = pd.concat([df, dummy_df], axis = 1)\n",
    "        return encoded\n",
    "\n",
    "    def country_tagger(df):\n",
    "        country_map = { 'Alittihad': 'emirates',\n",
    "                        'Echoroukonline': 'algeria',\n",
    "                        'Ryiadh': 'ksa',\n",
    "                        'SaudiYoum': 'ksa',\n",
    "                        'Techreen': 'syria',\n",
    "                        'Alqabas': 'kuwait',\n",
    "                        'Almustaqbal': 'lebanon',\n",
    "                        'Almasryalyoum': 'egypt',\n",
    "                        'Youm7': 'egypt',\n",
    "                        'Sabanews': 'yemen',\n",
    "                        }\n",
    "        df['country'] = df.source.map(country_map)\n",
    "        return df\n",
    "\n",
    "    def within_30_days(df_dateline, date):\n",
    "\n",
    "        if (df_dateline - date).days < 30 and (df_dateline - date).days > -30:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    important_dates = {\n",
    "        'september_11th': pd.to_datetime('09-11-2001'),\n",
    "        'capture_of_baghdad': pd.to_datetime('04-09-2003'),\n",
    "        'nick_berg': pd.to_datetime('05-12-2004'),\n",
    "        'iran_nulcear': pd.to_datetime('08-30-2006'),\n",
    "        'arab_spring': pd.to_datetime('12-20-2011')\n",
    "    }\n",
    "\n",
    "    for event, date in important_dates.items():\n",
    "        df[event] = df.dateline.apply(within_30_days, args = (date,))\n",
    "\n",
    "    df = country_tagger(df)\n",
    "\n",
    "\n",
    "    def encode_tags(df):\n",
    "\n",
    "        list_of_tags = []\n",
    "        for tag in df.tags.values:\n",
    "            list_of_tags.extend([val[1:-1] for val in tag[1:-1].split(', ')])\n",
    "\n",
    "        tag_list = list(set(list_of_tags))\n",
    "\n",
    "        for tag in tag_list:\n",
    "            df[tag] = 0\n",
    "\n",
    "        for i, tag in enumerate(df.tags):\n",
    "            for t in tag_list:\n",
    "                if t in tag:\n",
    "                    df[t].iloc[i] = 1\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    df = encode_tags(df)\n",
    "\n",
    "    df.head()\n",
    "    #print('encoding...')\n",
    "    encoded = encode_values(df, ['source', 'country'])\n",
    "    encoded.head()\n",
    "\n",
    "    #print('scaling...')\n",
    "    df['scaled_date'] = (df.dateline.astype('datetime64') - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    scaler.fit(df[['scaled_date']])\n",
    "\n",
    "    df['scaled_date'] = scaler.transform(df[['scaled_date']])\n",
    "    #print('encoding...')\n",
    "\n",
    "    #print('dropping non-encoded columns')\n",
    "\n",
    "    tags_to_drop = ['id', 'url', 'headline', 'dateline', 'text', 'tags', 'source', 'text_score', 'headline_label', 'headline_score', 'country']\n",
    "\n",
    "    encoded = encoded.rename(columns={'scaled_date': 'scaled_pub_date'})\n",
    "\n",
    "    encoded = encoded.drop(columns=tags_to_drop)\n",
    "    #print('splitting...')\n",
    "    encoded.text_label = encoded.text_label.map({'neutral': 0, 'negative': -1, 'positive': 1})\n",
    "\n",
    "    return encoded\n",
    "\n",
    "def model_train_val_test(encoded_df):\n",
    "    train, validate, test = split.train_validate_test_split(encoded_df, 'text_label')\n",
    "    clf = RandomForestClassifier(max_depth=14, min_samples_leaf=1)\n",
    "\n",
    "    x_train = train.drop(columns='text_label')\n",
    "    y_train = train['text_label']\n",
    "    \n",
    "    x_validate = validate.drop(columns='text_label')\n",
    "    y_validate = validate['text_label']\n",
    "\n",
    "    x_test = test.drop(columns='text_label')\n",
    "    y_test = test['text_label']\n",
    "\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    x_predic = clf.predict(x_test)\n",
    "    train_score = clf.score(x_train, x_train)\n",
    "    validate_score = clf.score(x_validate, y_validate)\n",
    "    test_score = clf.score(x_test, y_test)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'model': 'random_forests',\n",
    "        'depth': '14',\n",
    "        'min_sample_leaf': '1',\n",
    "        'train_acc': round(train_score * 100,1),\n",
    "        'validate_acc': round(validate_score * 100,1),\n",
    "        'test_acc': round(test_score * 100, 1)\n",
    "        })\n",
    "    \n",
    "    return results, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "833aaed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import regex as re\n",
    "from datetime import datetime\n",
    "\n",
    "import modeling as md\n",
    "import acquire\n",
    "import exploration\n",
    "import split\n",
    "import prepare\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "encoded = encode_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "153b671e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_label</th>\n",
       "      <th>ownership_status</th>\n",
       "      <th>september_11th</th>\n",
       "      <th>capture_of_baghdad</th>\n",
       "      <th>nick_berg</th>\n",
       "      <th>iran_nulcear</th>\n",
       "      <th>arab_spring</th>\n",
       "      <th>تشيني</th>\n",
       "      <th>أمريكا</th>\n",
       "      <th>القاعدة</th>\n",
       "      <th>...</th>\n",
       "      <th>source_Techreen</th>\n",
       "      <th>source_Youm7</th>\n",
       "      <th>country_algeria</th>\n",
       "      <th>country_egypt</th>\n",
       "      <th>country_emirates</th>\n",
       "      <th>country_ksa</th>\n",
       "      <th>country_kuwait</th>\n",
       "      <th>country_lebanon</th>\n",
       "      <th>country_syria</th>\n",
       "      <th>country_yemen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237701</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237702</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237703</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237704</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237705</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>237706 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_label  ownership_status  september_11th  capture_of_baghdad  \\\n",
       "0                0                 0               0                   0   \n",
       "1                0                 0               0                   0   \n",
       "2                0                 0               0                   0   \n",
       "3                0                 0               0                   0   \n",
       "4                1                 0               0                   0   \n",
       "...            ...               ...             ...                 ...   \n",
       "237701           1                 1               0                   0   \n",
       "237702           0                 1               0                   0   \n",
       "237703          -1                 1               0                   0   \n",
       "237704           0                 1               0                   0   \n",
       "237705           0                 1               0                   0   \n",
       "\n",
       "        nick_berg  iran_nulcear  arab_spring  تشيني  أمريكا  القاعدة  ...  \\\n",
       "0               0             0            0      0       0        0  ...   \n",
       "1               0             0            0      0       0        1  ...   \n",
       "2               0             0            0      0       0        0  ...   \n",
       "3               0             0            0      0       0        1  ...   \n",
       "4               0             0            0      0       0        1  ...   \n",
       "...           ...           ...          ...    ...     ...      ...  ...   \n",
       "237701          0             0            0      0       0        1  ...   \n",
       "237702          0             0            0      0       0        0  ...   \n",
       "237703          0             0            0      0       0        1  ...   \n",
       "237704          0             0            0      0       0        0  ...   \n",
       "237705          0             0            0      0       0        0  ...   \n",
       "\n",
       "        source_Techreen  source_Youm7  country_algeria  country_egypt  \\\n",
       "0                     0             0                0              0   \n",
       "1                     0             0                0              0   \n",
       "2                     0             0                0              0   \n",
       "3                     0             0                0              0   \n",
       "4                     0             0                0              0   \n",
       "...                 ...           ...              ...            ...   \n",
       "237701                0             0                0              0   \n",
       "237702                0             0                0              0   \n",
       "237703                0             0                0              0   \n",
       "237704                0             0                0              0   \n",
       "237705                0             0                0              0   \n",
       "\n",
       "        country_emirates  country_ksa  country_kuwait  country_lebanon  \\\n",
       "0                      1            0               0                0   \n",
       "1                      1            0               0                0   \n",
       "2                      1            0               0                0   \n",
       "3                      1            0               0                0   \n",
       "4                      1            0               0                0   \n",
       "...                  ...          ...             ...              ...   \n",
       "237701                 0            0               0                1   \n",
       "237702                 0            0               0                1   \n",
       "237703                 0            0               0                1   \n",
       "237704                 0            0               0                1   \n",
       "237705                 0            0               0                1   \n",
       "\n",
       "        country_syria  country_yemen  \n",
       "0                   0              0  \n",
       "1                   0              0  \n",
       "2                   0              0  \n",
       "3                   0              0  \n",
       "4                   0              0  \n",
       "...               ...            ...  \n",
       "237701              0              0  \n",
       "237702              0              0  \n",
       "237703              0              0  \n",
       "237704              0              0  \n",
       "237705              0              0  \n",
       "\n",
       "[237706 rows x 43 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b3be196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['source_Techreen', 'country_syria', 'source_Almasryalyoum', 'country_emirates', 'source_Alittihad', 'source_Youm7', 'source_SaudiYoum', 'country_yemen', 'source_Sabanews', 'country_ksa', 'capture_of_baghdad', 'ownership_status', 'source_Alqabas', 'country_kuwait', 'nick_berg', 'text_label']\n",
    "\n",
    "words = 'بوش, أمريكا, غوغل, تشيني, حرب الخليج'\n",
    "words = words.split(', ')\n",
    "features.extend(words)\n",
    "\n",
    "to_remove = ['country_syria', 'source_Alittihad', 'source_Sabanews']\n",
    "[features.remove(x) for x in to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e58492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0330671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cddaf0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def model_train_val_test(encoded_df):\n",
    "    train, validate, test = split.train_validate_test_split(encoded_df, 'text_label')\n",
    "    clf = RandomForestClassifier(max_depth=14, min_samples_leaf=1, random_state=0)\n",
    "\n",
    "    x_train = train.drop(columns='text_label')\n",
    "    y_train = train['text_label']\n",
    "    \n",
    "    x_validate = validate.drop(columns='text_label')\n",
    "    y_validate = validate['text_label']\n",
    "\n",
    "    x_test = test.drop(columns='text_label')\n",
    "    y_test = test['text_label']\n",
    "\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    train_score = clf.score(x_train, y_train)\n",
    "    validate_score = clf.score(x_validate, y_validate)\n",
    "    test_score = clf.score(x_test, y_test)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'model': 'random_forests',\n",
    "        'depth': '14',\n",
    "        'min_sample_leaf': '1',\n",
    "        'train_acc': round(train_score * 100,1),\n",
    "        'validate_acc': round(validate_score * 100,1),\n",
    "        'test_acc': round(test_score * 100, 1)\n",
    "        }, index=range(1))\n",
    "    \n",
    "    return results, clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1ff7b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>depth</th>\n",
       "      <th>min_sample_leaf</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>validate_acc</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>random_forests</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>73.9</td>\n",
       "      <td>73.9</td>\n",
       "      <td>73.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            model depth min_sample_leaf  train_acc  validate_acc  test_acc\n",
       "0  random_forests    14               1       73.9          73.9      73.9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_and_model():\n",
    "    encoded = encode_data()\n",
    "\n",
    "    features = ['source_Techreen', 'country_syria', 'source_Almasryalyoum', 'country_emirates', 'source_Alittihad', 'source_Youm7', 'source_SaudiYoum', 'country_yemen', 'source_Sabanews', 'country_ksa', 'capture_of_baghdad', 'ownership_status', 'source_Alqabas', 'country_kuwait', 'nick_berg', 'text_label']\n",
    "\n",
    "    words = 'بوش, أمريكا, غوغل, تشيني, حرب الخليج'\n",
    "    words = words.split(', ')\n",
    "    features.extend(words)\n",
    "\n",
    "    to_remove = ['country_syria', 'source_Alittihad', 'source_Sabanews']\n",
    "    [features.remove(x) for x in to_remove]\n",
    "\n",
    "\n",
    "    results, clf = model_train_val_test(encoded[features])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7347dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source_Techreen': 0.3120104383896887,\n",
       " 'source_Almasryalyoum': 0.07678653480385358,\n",
       " 'country_emirates': 0.036817746909751294,\n",
       " 'source_Youm7': 0.07978411103008143,\n",
       " 'source_SaudiYoum': 0.028452966839296375,\n",
       " 'country_yemen': 0.027549055815506134,\n",
       " 'country_ksa': 0.04834361354080233,\n",
       " 'capture_of_baghdad': 0.017122953378165957,\n",
       " 'ownership_status': 0.05540847964825931,\n",
       " 'source_Alqabas': 0.005576187362146184,\n",
       " 'country_kuwait': 0.005112449417139919,\n",
       " 'nick_berg': 0.008195269145295128,\n",
       " 'بوش': 0.05363689134180812,\n",
       " 'أمريكا': 0.2160629307262095,\n",
       " 'غوغل': 0.005475695031461952,\n",
       " 'تشيني': 0.010847001861394855,\n",
       " 'حرب الخليج': 0.012817674759139277}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(clf.feature_names_in_, clf.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d74e65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dab4eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eb7bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be614666",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = split.train_validate_test_split(encoded[features], 'text_label')\n",
    "\n",
    "x_train = train.drop(columns='text_label')\n",
    "y_train = train['text_label']\n",
    "\n",
    "clf.decision_path(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c5ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e55da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.remove('text_label')\n",
    "dict(zip(list(clf.feature_importances_), features))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc86782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.n_features_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42f33da",
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c79483",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoded[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e5e888",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.remove('text_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c2d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "\n",
    "for weight in clf.feature_importances_:\n",
    "    for feat in features:\n",
    "        scores[feat] = weight\n",
    "        break\n",
    "    \n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5573d81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.text_label = encoded.text_label.map({'neutral': 0, 'negative': -1, 'positive': 1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd2faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.text_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8743ab3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.text_label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7b644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b60923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036515f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "md.decision_tree(train, validate, test, 'text_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d459aa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 day windows on both ends does not improve model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a8526",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70cdb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded[features].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea63adc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3def0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['source_Techreen', 'country_syria', 'source_Almasryalyoum', 'country_emirates', 'source_Alittihad', 'source_Youm7', 'source_SaudiYoum', 'country_yemen', 'source_Sabanews', 'country_ksa', 'capture_of_baghdad', 'ownership_status', 'source_Alqabas', 'country_kuwait', 'nick_berg', 'text_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe9f8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714e22b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['source_Techreen', 'country_syria', 'source_Almasryalyoum', 'country_emirates', 'source_Alittihad', 'source_Youm7', 'source_SaudiYoum', 'country_yemen', 'source_Sabanews', 'country_ksa', 'capture_of_baghdad', 'ownership_status', 'source_Alqabas', 'country_kuwait', 'nick_berg', 'text_label']\n",
    "\n",
    "words = 'بوش, أمريكا, غوغل, تشيني, حرب الخليج'\n",
    "words = words.split(', ')\n",
    "features.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b63fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6943f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af438a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.remove('America')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b2faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5da34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "x_train = train.drop(columns='text_label')\n",
    "y_train = train['text_label']\n",
    "\n",
    "x_validate = validate.drop(columns='text_label')\n",
    "y_validate = validate['text_label']\n",
    "\n",
    "clf = SVC(kernel='poly', degree=2)\n",
    "clf.fit(x_train, y_train)\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.5f}'\n",
    "      .format(clf.score(x_train, y_train)))\n",
    "print('Accuracy of Decision Tree classifier on val set: {:.5f}'\n",
    "      .format(clf.score(x_validate, y_validate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022ee742",
   "metadata": {},
   "outputs": [],
   "source": [
    ",Var,Scores\n",
    "35,source_Techreen,8943.448879256535\n",
    "42,country_Syria,8943.448879256423\n",
    "28,source_Almasryalyoum,2355.0164748339\n",
    "10,America,1749.3620117708529\n",
    "43,country_UAE,1254.1639088750942\n",
    "27,source_Alittihad,1254.1639088750908\n",
    "36,source_Youm7,1254.1038226182388\n",
    "23,scaled_date,1028.351322645141\n",
    "21,Bush,825.5788927612839\n",
    "34,source_SaudiYoum,563.4215943320206\n",
    "44,country_Yemen,532.3199952120876\n",
    "33,source_Sabanews,532.3199952120876\n",
    "41,country_Saudi_Arabia,359.4754705031025\n",
    "1,capt_bag,272.2506389745735\n",
    "7,Google,239.54317229043158\n",
    "26,ownership_status,223.20219347351065\n",
    "30,source_Alqabas,176.74692556317044\n",
    "39,country_Kuwait,176.74692556317044\n",
    "6,Gulf War,130.54652729478855\n",
    "9,Cheney,124.84083618600735\n",
    "2,nick,123.55177378850968\n",
    "\n",
    "features = ['source_Techreen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b7fba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import date_fixer\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "from camel_tools.sentiment import SentimentAnalyzer\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "topics = {'America' : 'أمريكا',\n",
    "            'American' : 'أمريكيّ',\n",
    "            'American (f)' : 'أمريكيّة',\n",
    "            'American (pl)' : 'أمريكيّين',\n",
    "            'The United States' : 'الولايات المتحدة',\n",
    "            'The United States' : 'دول موحّدة',\n",
    "            'Washington' : 'واشنطن',\n",
    "            'Bush' : 'بوش',\n",
    "            'Obama' : 'أوباما',\n",
    "            'Cheney' : 'تشيني',\n",
    "            'Clinton' : 'كلينتون',\n",
    "            'Osama Bin Laden' : 'أسامة بن لادن',\n",
    "            'Al Gore' : 'آل غور',\n",
    "            'World Trade Center' : 'مركز التجارة العالمي',\n",
    "            '9/11' : '9/11',\n",
    "            'September 11' : '11 سبتمبر',\n",
    "            'Gulf War' : 'حرب الخليج',\n",
    "            'Google' : 'غوغل',\n",
    "            'Facebook' : 'فيسبوك',\n",
    "            'Al Qaida' : 'القاعدة'}\n",
    "\n",
    "\n",
    "def flip_key_value_pairs(dicts):\n",
    "    res = dict((v,k) for k,v in dicts.items())\n",
    "    return res\n",
    "\n",
    "def make_eng_tags(df_tags):\n",
    "    eng_tags =[]\n",
    "    rev_topics = flip_key_value_pairs(topics)\n",
    "    for key, value in rev_topics.items():\n",
    "        if key in df_tags:\n",
    "            eng_tags.append(value)\n",
    "    return eng_tags\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Change name to which file you're doing sentiment analysis on, NOT including the '.csv' \n",
    "Output file will be 'labeled_<filename>.csv'\n",
    "\"\"\"\n",
    "\n",
    "msa = pipeline('text-classification', model=\"CAMeL-Lab/bert-base-arabic-camelbert-msa-sentiment\")\n",
    "\n",
    "\n",
    "\n",
    "def load_and_label_df(name):\n",
    "    path = 'split_articles/'\n",
    "    df = load_csv(path + name)\n",
    "    print(f'loaded {name}')\n",
    "    print('labeling/scoring...')\n",
    "    df = create_labels_scores(df, name)\n",
    "    print('done labeling/scoring!')\n",
    "    return df\n",
    "\n",
    "def load_csv(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    return df\n",
    "\n",
    "def make_msa(df_text):\n",
    "    \n",
    "    try:\n",
    "        done = msa(df_text)\n",
    "        return done\n",
    "    except:\n",
    "        \n",
    "        try:\n",
    "            first_half = msa(df_text[:round(len(df_text)/2)]) \n",
    "            second_half = msa(df_text[round(len(df_text)/2):])\n",
    "            if first_half[0]['label'] == second_half[0]['label']:\n",
    "                label = first_half[0]['label']\n",
    "                score = (test_1[0]['score'] + test_2[0]['score'])/2\n",
    "            done = [{'label': label, 'score': score}]\n",
    "            return done\n",
    "        except:\n",
    "        \n",
    "            try:\n",
    "                beginning = msa(df_text[:round(len(df_text)/3)]) \n",
    "                middle = msa(df_text[round(len(df_text)/3):round(len(df_text)*2/3)])\n",
    "                end = msa(df_text[round(len(df_text)*2/3):])\n",
    "\n",
    "                if (beginning[0]['label'] == middle[0]['label']) and (beginning[0]['label'] == end[0]['label']):\n",
    "                    label = first_half[0]['label']\n",
    "                    score = (beginning[0]['score'] + middle[0]['score'] + end[0]['score'])/3\n",
    "                    done = [{'label': label, 'score': score}]\n",
    "                    return done\n",
    "                else:\n",
    "                    return 'sentiment of parts not equal'\n",
    "            except:\n",
    "                return '3 is not enough'\n",
    "        \n",
    "def analyze_text(df):\n",
    "    scores = []\n",
    "    print('analyzing_texts')\n",
    "    scores = [make_msa(val) for val in df.text.values]\n",
    "    return scores\n",
    "\n",
    "def analyze_headline(df):\n",
    "    print('analyzing headlines')\n",
    "    headline_scores = [make_msa(val) for val in df.headline.values]\n",
    "    return headline_scores\n",
    "\n",
    "def label_and_scores(msa_scores):\n",
    "    labels = []\n",
    "    scores = []\n",
    "    for val in msa_scores:\n",
    "        try:\n",
    "            labels.append(val[0]['label'])\n",
    "            scores.append(val[0]['score'])\n",
    "        except:\n",
    "            labels.append(False)\n",
    "            scores.append(False)\n",
    "\n",
    "    return labels, scores\n",
    "\n",
    "def create_labels_scores(df, name):\n",
    "    text_scores = analyze_text(df)\n",
    "    labels, scores = label_and_scores(text_scores)\n",
    "    df['text_label'] = labels\n",
    "    df['text_score'] = scores\n",
    "\n",
    "    headline_scores = analyze_headline(df)\n",
    "    labels, scores = label_and_scores(headline_scores)\n",
    "    df['headline_label'] = labels\n",
    "    df['headline_score'] = scores\n",
    "\n",
    "    # CHANGE 'BLOCK_NAME' TO WHATEVER YOU WANT\n",
    "    df.to_csv('labeled_split_articles/labeled_'+ name + '.csv', index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "def split_tons_of_csvs():\n",
    "    df = pd.read_csv('C:/Users/kyleg/codeup/all_articles.csv')\n",
    "    \n",
    "    end_point = 'C:/Users/kyleg/codeup/split_articles/'\n",
    "    \n",
    "    df_shape = df.shape[0]\n",
    "    \n",
    "    one_thousandth = int(df_shape/1000)\n",
    "    \n",
    "    for i in range(0, df_shape, one_thousandth):\n",
    "        copy = df.iloc[i:i+one_thousandth].copy()\n",
    "        copy.to_csv(end_point+str(i) + '.csv', index=False)\n",
    "        \n",
    "\n",
    "def make_random_indexes(list_of_files):\n",
    "    unique_ = []\n",
    "    length = len(list_of_files)\n",
    "    unique_indexes= np.random.randint(0, length, size=length**2 , dtype=int)\n",
    "    [unique_.append(val) for val in unique_indexes if val not in unique_]\n",
    "\n",
    "    return unique_\n",
    "\n",
    "\n",
    "def split_and_process_articles():\n",
    "    labeled = 'labeled_split_articles'\n",
    "    unlabeled = 'split_articles'\n",
    "\n",
    "    os.listdir(labeled)\n",
    "\n",
    "    unlabeled_files = os.listdir(unlabeled)\n",
    "    labeled_files = []\n",
    "\n",
    "    for path in os.listdir(labeled):\n",
    "        labeled_files.append(re.search('\\d*\\.', path).group()[:-1])\n",
    "\n",
    "    print(labeled_files)\n",
    "\n",
    "\n",
    "    ul_files = unlabeled_files[:]\n",
    "    files_found  = []\n",
    "    for p in labeled_files:\n",
    "        for pth in unlabeled_files:\n",
    "            if p+'.csv' == pth:\n",
    "                ul_files.remove(pth)\n",
    "\n",
    "    path = 'split_articles/'\n",
    "    unique_indexes  = make_random_indexes(ul_files)\n",
    "    print(unique_indexes)\n",
    "    for i in unique_indexes:\n",
    "        load_and_label_df(ul_files[i])\n",
    "\n",
    "def within_30_days(df_dateline, date):\n",
    "    \n",
    "    if (df_dateline - date).days < 30 and (df_dateline - date).days > -30:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def create_date_features(df):\n",
    "    important_dates = {\n",
    "        'september_11th': pd.to_datetime('09-11-2001'),\n",
    "        'capture_of_baghdad': pd.to_datetime('04-09-2003'),\n",
    "        'nick_berg': pd.to_datetime('05-12-2004'),\n",
    "        'iran_nulcear': pd.to_datetime('08-30-2006'),\n",
    "        'arab_spring': pd.to_datetime('12-20-2011')\n",
    "    }\n",
    "\n",
    "    for event, date in important_dates.items():\n",
    "        df[event] = df.dateline.apply(within_30_days)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def update_csv():\n",
    "    waiting = False\n",
    "\n",
    "    while True:\n",
    "        time.sleep(5)\n",
    "        curr_time = pd.Timestamp.now()\n",
    "\n",
    "        if curr_time.hour % 2 != 0:\n",
    "            waiting = False\n",
    "\n",
    "        if curr_time.hour % 2 == 0 and not waiting:\n",
    "            print('updating csv...')\n",
    "            all_dfs = []\n",
    "            path = 'C:/Users/kyleg/codeup/labeled_split_articles/'\n",
    "            parent = 'E:/drive/arabic_NLP/'\n",
    "            for fi in os.listdir(path):\n",
    "                all_dfs.append(pd.read_csv(path + fi))\n",
    "\n",
    "            daily_csv = pd.concat(all_dfs).reset_index()\n",
    "            print('fixing dates...')\n",
    "            daily_csv = date_fixer.make_datetime(daily_csv)\n",
    "            print('writing csv...')\n",
    "            daily_csv.to_csv(parent + f'daily{curr_time.month}_{curr_time.day}.csv', index=False)\n",
    "            print(f'csv written. updating in google drive as of {curr_time.hour}:{curr_time.min}')\n",
    "            waiting = True\n",
    "    \n",
    "def techreen_date(date_entry):\n",
    "    '''\n",
    "    This function takes in a date entry and applies a regex search to isolate just the relevant date info\n",
    "    and returns it out.\n",
    "    '''\n",
    "    exp = r'(\\d?\\d)/(\\d?\\d)/(\\d\\d\\d\\d)'\n",
    "    match = re.search(exp, str(date_entry))\n",
    "    day =  match[1]\n",
    "    month = match[2]\n",
    "    year = match[3]\n",
    "    date = year+ '-' + month + '-' + day\n",
    "    return date\n",
    "\n",
    "def saudiyoum_date(date_entry):\n",
    "    '''\n",
    "    This function takes in a date entry and applies a regex search to isolate just the relevant date info\n",
    "    and returns it out.\n",
    "    '''\n",
    "    try:\n",
    "        exp = r'(2\\d\\d\\d)-(\\d?\\d)-(\\d?\\d)'\n",
    "        match = re.search(exp, str(date_entry))\n",
    "        day = match[3]\n",
    "        month = match[2]\n",
    "        year = match[1]\n",
    "        date = year+ '-' + month + '-' + day\n",
    "        return date\n",
    "    except:\n",
    "        exp = r'(\\d\\d\\d\\d)/(\\d?\\d)/(\\d?\\d)'\n",
    "        match = re.search(exp, str(date_entry))\n",
    "        day =  match[3]\n",
    "        month = match[2]\n",
    "        year = match[1]\n",
    "        date = year+ '-' + month + '-' + day\n",
    "        return date\n",
    "\n",
    "def youm7_date(date_entry, months_map):\n",
    "    exp = r'(\\d?\\d)(.*)(\\d\\d\\d\\d)'\n",
    "    match = re.search(exp, date_entry)\n",
    "    day =  match[1]\n",
    "    month = match[2].strip()\n",
    "    year = match[3]\n",
    "    month = months_map[month]\n",
    "    date = year+ '-' + month + '-' + day\n",
    "    return date\n",
    "\n",
    "def alittihad_date(date_entry, months_map):\n",
    "    exp = r'(\\d?\\d)(.*)(\\d\\d\\d\\d)'\n",
    "    match = re.search(exp, date_entry)\n",
    "    day =  match[1]\n",
    "    month = match[2].strip()\n",
    "    year = match[3]\n",
    "    month = months_map[month]\n",
    "    date = year+ '-' + month + '-' + day\n",
    "    return date\n",
    "\n",
    "def almustaqbal_date(date_entry):\n",
    "    arabic_months_map = {   'كانون الثاني':'01',\n",
    "                            'شباط':'02',\n",
    "                            'آذار':'03',\n",
    "                            'نيسان':'04',\n",
    "                            'أيار':'05',\n",
    "                            'حزيران':'06',\n",
    "                            'تموز':'07',\n",
    "                            'آب':'08',\n",
    "                            'أيلول':'09',\n",
    "                            'تشرين الأول':'10',\n",
    "                            'تشرين الثاني':'11',\n",
    "                            'كانون الأول':'12'}\n",
    "    exp = r'(\\d?\\d)\\s(.*)\\s(\\d\\d\\d\\d).+العدد'\n",
    "    match = re.search(exp, date_entry)\n",
    "    day =  match[1]\n",
    "    month = match[2].strip()\n",
    "    year = match[3]\n",
    "    month = arabic_months_map[month]\n",
    "    date = year+ '-' + month + '-' + day\n",
    "    return date\n",
    "\n",
    "def ryiadh_date(date_entry, months_map):\n",
    "    try:\n",
    "        exp = r'-\\s?(\\d?\\d)\\s?(.+)\\s?(\\d\\d\\d\\d)\\s?م'\n",
    "        match = re.search(exp, date_entry)\n",
    "        day =  match[1]\n",
    "        month = match[2].strip()\n",
    "        year = match[3]\n",
    "        month = months_map[month]\n",
    "        date = year+ '-' + month + '-' + day\n",
    "        return date\n",
    "\n",
    "    except:\n",
    "        try:\n",
    "            exp = r'(\\d?\\d)/(\\d?\\d)/(\\d\\d\\d\\d)'\n",
    "            match = re.search(exp, date_entry)\n",
    "            day =  match[1]\n",
    "            month = match[2].strip()\n",
    "            year = match[3]\n",
    "            date = year+ '-' + month + '-' + day\n",
    "            return date\n",
    "        except:\n",
    "            try:\n",
    "                exp = r'(\\d\\d\\d\\d)-(\\d?\\d)-(\\d?\\d)'\n",
    "                match = re.search(exp, date_entry)\n",
    "                day =  match[3]\n",
    "                month = match[2].strip()\n",
    "                year = match[1]\n",
    "                date = year+ '-' + month + '-' + day\n",
    "                return date\n",
    "            except:\n",
    "                return pd.NaT\n",
    "\n",
    "def alqabas_date(date_entry):\n",
    "    exp = r'(\\d\\d\\d\\d)/(\\d?\\d)/(\\d?\\d)'\n",
    "    match = re.search(exp, date_entry)\n",
    "    day =  match[3]\n",
    "    month = match[2]\n",
    "    year = match[1]\n",
    "    date = year+ '-' + month + '-' + day\n",
    "    return date\n",
    "\n",
    "def almasryalyoum_date(date_entry):\n",
    "    exp = r'(\\d?\\d)/(\\d?\\d)/(\\d\\d\\d\\d)'\n",
    "    match = re.search(exp, date_entry)\n",
    "    day =  match[1]\n",
    "    month = match[2]\n",
    "    year = match[3]\n",
    "    date = year+ '-' + month + '-' + day\n",
    "    return date\n",
    "\n",
    "def sabanews_date(date_entry, months_map):\n",
    "    exp = r'(\\d?\\d)/(.+)/(\\d\\d\\d\\d)'\n",
    "    match = re.search(exp, date_entry)\n",
    "    day =  match[1]\n",
    "    month = match[2].strip()\n",
    "    month = months_map[month]\n",
    "    year = match[3]\n",
    "    date = year+ '-' + month + '-' + day\n",
    "    return date\n",
    "\n",
    "def echoroukonline_date(date_entry):\n",
    "    exp = r'(\\d\\d\\d\\d)/(\\d?\\d)/(\\d?\\d)'\n",
    "    match = re.search(exp, date_entry)\n",
    "    day =  match[3]\n",
    "    month = match[2]\n",
    "    year = match[1]\n",
    "    date = year+ '-' + month + '-' + day\n",
    "    return date\n",
    "\n",
    "def make_datetime(dataframe):\n",
    "    # This function works, ignore the one above\n",
    "    df = dataframe.dropna()\n",
    "    months_map = {'يناير':'01',\n",
    "        'ينابر':'01',\n",
    "        'فبراير':'02',\n",
    "        'مارس':'03',\n",
    "        'أبريل':'04',\n",
    "        'ابريل':'04',\n",
    "        'مايو':'05',\n",
    "        'يونيو':'06',\n",
    "        'يوليو':'07',\n",
    "        'أغسطس':'08',\n",
    "        'اغسطس':'08',\n",
    "        'سبتمبر':'09',\n",
    "        'أكتوبر':'10',\n",
    "        'اكتوبر':'10',\n",
    "        'نوفمبر':'11',\n",
    "        'ديسمبر':'12',\n",
    "        'إبريل':'04',\n",
    "        'ماي':'05',\n",
    "        'يونيه':'06',\n",
    "        'يوليه':'07'}\n",
    "    m_maps = months_map\n",
    "    date_list = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "\n",
    "        if df.iloc[i].source == 'SaudiYoum':\n",
    "            date_list.append(saudiyoum_date(df.iloc[i]['dateline']))\n",
    "            #print(df.id.iloc[i] + ' successful')\n",
    "            \n",
    "        elif df.iloc[i].source == 'Techreen':\n",
    "            date_list.append(techreen_date(df.iloc[i]['dateline']))\n",
    "            #print(df.id.iloc[i] + ' successful')\n",
    "\n",
    "        elif df.iloc[i].source == 'Youm7':\n",
    "            date_list.append(youm7_date(df.iloc[i]['dateline'], m_maps))\n",
    "            #print(df.id.iloc[i] + ' successful')\n",
    "\n",
    "        elif df.iloc[i].source == 'Alittihad':\n",
    "            date_list.append(alittihad_date(df.iloc[i]['dateline'], m_maps))\n",
    "            #print(df.id.iloc[i] + ' successful')\n",
    "\n",
    "        elif df.iloc[i].source == 'Almustaqbal':\n",
    "            date_list.append(almustaqbal_date(df.iloc[i]['dateline']))\n",
    "            #print(df.id.iloc[i] + ' successful')\n",
    "\n",
    "        elif df.iloc[i].source == 'Ryiadh':\n",
    "            date_list.append(ryiadh_date(df.iloc[i]['dateline'], m_maps))\n",
    "            #print(df.id.iloc[i] + ' successful')\n",
    "\n",
    "        elif df.iloc[i].source == 'Alqabas':\n",
    "            date_list.append(alqabas_date(df.iloc[i]['dateline']))\n",
    "            #print(df.id.iloc[i] + ' successful')\n",
    "\n",
    "        elif df.iloc[i].source == 'Almasryalyoum':\n",
    "            date_list.append(almasryalyoum_date(df.iloc[i]['dateline']))\n",
    "            #print(df.id.iloc[i] + ' successful')\n",
    "\n",
    "        elif df.iloc[i].source == 'Sabanews':\n",
    "            date_list.append(sabanews_date(df.iloc[i]['dateline'], m_maps))\n",
    "            #print(df.id.iloc[i] + ' successful')\n",
    "            \n",
    "        elif df.iloc[i].source == 'Echoroukonline':\n",
    "            date_list.append(echoroukonline_date(df.iloc[i]['dateline']))\n",
    "            #print(df.id.iloc[i] + ' successful')\n",
    "    df['dateline'] = pd.to_datetime(date_list)\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def using_ahocorasick(col, lst):\n",
    "    A = ahocorasick.Automaton(ahocorasick.STORE_INTS)\n",
    "    for word in lst:\n",
    "        A.add_word(word.lower())\n",
    "    A.make_automaton() \n",
    "    col = col.astype(str)\n",
    "    col = col.str.lower()\n",
    "    mask = col.apply(lambda x: bool(list(A.iter(x))))\n",
    "    tags = col.apply(lambda x: list(A.iter(x)))\n",
    "    return mask, tags\n",
    "\n",
    "def look_for_words_in_text(df_text):\n",
    "    \n",
    "    topics = {'America' : 'أمريكا',\n",
    "            'American' : 'أمريكيّ',\n",
    "            'American (f)' : 'أمريكيّة',\n",
    "            'American (pl)' : 'أمريكيّين',\n",
    "            'The United States' : 'الولايات المتحدة',\n",
    "            'The United States' : 'دول موحّدة',\n",
    "            'Washington' : 'واشنطن',\n",
    "            'Bush' : 'بوش',\n",
    "            'Obama' : 'أوباما',\n",
    "            'Cheney' : 'تشيني',\n",
    "            'Clinton' : 'كلينتون',\n",
    "            'Osama Bin Laden' : 'أسامة بن لادن',\n",
    "            'Al Gore' : 'آل غور',\n",
    "            'World Trade Center' : 'مركز التجارة العالمي',\n",
    "            '9/11' : '9/11',\n",
    "            'September 11' : '11 سبتمبر',\n",
    "            'Gulf War' : 'حرب الخليج',\n",
    "            'Google' : 'غوغل',\n",
    "            'Facebook' : 'فيسبوك',\n",
    "            'Al Qaida' : 'القاعدة'}\n",
    "    \n",
    "    topics = flip_key_value_pairs(topics)\n",
    "    \n",
    "    tags = []\n",
    "    for key in topics.keys():\n",
    "        if key in df_text:\n",
    "            tags.append(key)\n",
    "            \n",
    "    return tags\n",
    "\n",
    "def make_relevant_tagged_df(df):\n",
    "    topics = {'America' : 'أمريكا',\n",
    "            'American' : 'أمريكيّ',\n",
    "            'American (f)' : 'أمريكيّة',\n",
    "            'American (pl)' : 'أمريكيّين',\n",
    "            'The United States' : 'الولايات المتحدة',\n",
    "            'The United States' : 'دول موحّدة',\n",
    "            'Washington' : 'واشنطن',\n",
    "            'Bush' : 'بوش',\n",
    "            'Obama' : 'أوباما',\n",
    "            'Cheney' : 'تشيني',\n",
    "            'Clinton' : 'كلينتون',\n",
    "            'Osama Bin Laden' : 'أسامة بن لادن',\n",
    "            'Al Gore' : 'آل غور',\n",
    "            'World Trade Center' : 'مركز التجارة العالمي',\n",
    "            '9/11' : '9/11',\n",
    "            'September 11' : '11 سبتمبر',\n",
    "            'Gulf War' : 'حرب الخليج',\n",
    "            'Google' : 'غوغل',\n",
    "            'Facebook' : 'فيسبوك',\n",
    "            'Al Qaida' : 'القاعدة'}\n",
    "\n",
    "\n",
    "    topics = flip_key_value_pairs(topics)\n",
    "    mask, tags = using_ahocorasick(df.text, list(topics.keys()))\n",
    "\n",
    "    copied = df[mask].copy()\n",
    "\n",
    "    return copied, tags\n",
    "\n",
    "def ramadan(df):\n",
    "    date_list = []\n",
    "    for i in range(len(df)):\n",
    "        if (\n",
    "        (df.dateline[i] >=dt.datetime(2001, 11, 17)) & (df.dateline[i] <= dt.datetime(2001, 12, 16))\n",
    "        or (df.dateline[i] >=dt.datetime(2002, 11, 6)) & (df.dateline[i] <= dt.datetime(2002, 12, 5))\n",
    "        or (df.dateline[i] >=dt.datetime(2003, 10, 27)) & (df.dateline[i] <= dt.datetime(2003, 11, 25))\n",
    "        or (df.dateline[i] >=dt.datetime(2004, 10, 16)) & (df.dateline[i] <= dt.datetime(2004, 11, 13))\n",
    "        or (df.dateline[i] >=dt.datetime(2005, 10, 5)) & (df.dateline[i] <= dt.datetime(2005, 11, 2))\n",
    "        or (df.dateline[i] >=dt.datetime(2006, 9, 24)) & (df.dateline[i] <= dt.datetime(2006, 10, 23))\n",
    "        or (df.dateline[i] >=dt.datetime(2007, 9, 13)) & (df.dateline[i] <= dt.datetime(2007, 10, 12))\n",
    "        or (df.dateline[i] >=dt.datetime(2008, 9, 2)) & (df.dateline[i] <= dt.datetime(2008, 10, 1))\n",
    "        or (df.dateline[i] >=dt.datetime(2009, 8, 22)) & (df.dateline[i] <= dt.datetime(2009, 9, 20))\n",
    "        or (df.dateline[i] >=dt.datetime(2010, 8, 11)) & (df.dateline[i] <= dt.datetime(2010, 9, 9))\n",
    "        or (df.dateline[i] >=dt.datetime(2011, 8, 1)) & (df.dateline[i] <= dt.datetime(2011, 8, 30))\n",
    "        or (df.dateline[i] >=dt.datetime(2012, 7, 20)) & (df.dateline[i] <= dt.datetime(2012, 8, 18))\n",
    "        or (df.dateline[i] >=dt.datetime(2013, 7, 9)) & (df.dateline[i] <= dt.datetime(2013, 8, 7))\n",
    "        or (df.dateline[i] >=dt.datetime(2014, 6, 29)) & (df.dateline[i] <= dt.datetime(2014, 7, 28))):\n",
    "            date_list.append(1)\n",
    "        else:\n",
    "            date_list.append(0)\n",
    "        \n",
    "    return date_list\n",
    "\n",
    "def encode_tags(df):\n",
    "\n",
    "    list_of_tags = []\n",
    "    for tag in df.tags.values:\n",
    "        list_of_tags.extend([val[1:-1] for val in tag[1:-1].split(', ')])\n",
    "\n",
    "    tag_list = list(set(list_of_tags))\n",
    "\n",
    "    for tag in tag_list:\n",
    "        df[tag] = 0\n",
    "\n",
    "    for i, tag in enumerate(df.tags):\n",
    "        for t in tag_list:\n",
    "            if t in tag:\n",
    "                df[t].iloc[i] = 1\n",
    "                \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_encoded_df(df):\n",
    "    '''Creates an encoded df from the original df, including bigrams and trigrams, that is \n",
    "    compatible with modeling.'''\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1,3))\n",
    "    tfidfs = tfidf.fit_transform(df.dropna().lemm.values)\n",
    "\n",
    "    tfidf_df = pd.DataFrame(tfidfs.todense(), columns=tfidf.get_feature_names())\n",
    "    col = pd.DataFrame({'programming_language_99': df.dropna().reset_index().drop(columns='index').language.values})\n",
    "\n",
    "    encoded_df = pd.concat([tfidf_df, col], axis=1)\n",
    "\n",
    "    return encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab8e9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
